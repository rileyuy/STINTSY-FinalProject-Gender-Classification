{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gender Classification using Logistic Regression Model\n",
    "\n",
    "The main objective of this notebook is to see how logistic regression is used together along with a gender classification dataset that contains about 58,000 male and female images that are "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements from Rubric (dont delete muna)\n",
    "\n",
    "An overview or description of the data is provided, including how it was collected, and its implications on the types of conclusions that could be made from the data. A description of the variables, observations, and/or structure of the data is provided. The target task is well introduced and clearly defined.\n",
    "\n",
    "The data is sufficiently explored to get a grasp of the distribution and the content of the data. Appropriate summaries and visualizations are presented. Insights into how the EDA can help the model training is mentioned.\n",
    "\n",
    "The necessary steps for preprocessing and cleaning are performed, including explanations for every step. If no preprocessing or cleaning is done, there is a justification on why it was not needed.\n",
    "\n",
    "The appropriate models are used to accomplish the machine learning task. Justification of choosing the models is shown.\n",
    "\n",
    "Appropriate data-driven error analysis is made, and changes to the model selection and hyperparameters are performed to improve model performance. The study exhausts improvements that can be done to the model\n",
    "\n",
    "The study is concluded by effectively summarizing the efforts of the authors. Recommendations on how the model could be further improved are provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "\n",
    "The Gender Classification Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47009\n",
      "11649\n",
      "Found 0 images belonging to 0 classes.\n",
      "Found 0 images belonging to 0 classes.\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "import tensorflow as tf\n",
    "\n",
    "# Image transformations\n",
    "image_transforms = {\n",
    "    # Train uses data augmentation\n",
    "    'train':\n",
    "    transforms.Compose([\n",
    "        transforms.RandomResizedCrop(size=256, scale=(0.8, 1.0)),\n",
    "        transforms.RandomRotation(degrees=15),\n",
    "        transforms.ColorJitter(),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.CenterCrop(size=224),  # Image net standards\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])  # Imagenet standards\n",
    "    ]),\n",
    "    # Validation does not use augmentation\n",
    "    'valid':\n",
    "    transforms.Compose([\n",
    "        transforms.Resize(size=256),\n",
    "        transforms.CenterCrop(size=224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "# Datasets from folders\n",
    "\n",
    "# tm_path = \"C:\\Users\\uyjus\\Documents\\GitHub\\STINTSY-FinalProject-Gender-Classification\\train\\female\"\n",
    "\n",
    "# data = {\n",
    "#     'train_male':\n",
    "#     datasets.ImageFolder(root='train/train_female', transform=image_transforms['train']),\n",
    "#     'train_female':\n",
    "#     datasets.ImageFolder(root='train/train_male', transform=image_transforms['train']),\n",
    "#     'validation_male':\n",
    "#     datasets.ImageFolder(root='valid/valid_male', transform=image_transforms['valid']),\n",
    "#     'validation_female':\n",
    "#     datasets.ImageFolder(root='valid/valid_female', transform=image_transforms['valid']),\n",
    "# }\n",
    "\n",
    "data = {\n",
    "    'training':\n",
    "    datasets.ImageFolder(root='train/train_data', transform=image_transforms['train']),\n",
    "    'validation':\n",
    "    datasets.ImageFolder(root='valid/valid_data', transform=image_transforms['valid']),\n",
    "}\n",
    "\n",
    "# tm_batch_size=len(data[\"train_male\"])\n",
    "# print (tm_batch_size)\n",
    "# tf_batch_size=len(data[\"train_female\"])\n",
    "# print (tf_batch_size)\n",
    "# vm_batch_size=len(data[\"validation_male\"])\n",
    "# print (vm_batch_size)\n",
    "# vf_batch_size=len(data[\"validation_female\"])\n",
    "# print (vf_batch_size)\n",
    "\n",
    "training_size=len(data[\"training\"])\n",
    "print (training_size)\n",
    "validation_size=len(data[\"validation\"])\n",
    "print (validation_size)\n",
    "\n",
    "# # Dataloader iterators, make sure to shuffle\n",
    "# dataloaders = {\n",
    "#     'train_male': DataLoader(data['train_male'], batch_size=tm_batch_size, shuffle=True),\n",
    "#     'train_female': DataLoader(data['train_female'], batch_size=tf_batch_size, shuffle=True),\n",
    "#     'validation_male': DataLoader(data['validation_male'], batch_size=vm_batch_size, shuffle=True),\n",
    "#     'validation_female': DataLoader(data['validation_female'], batch_size=vf_batch_size, shuffle=True)\n",
    "# }\n",
    "\n",
    "dataloaders = {\n",
    "    'training': DataLoader(data['training'], batch_size=tm_batch_size, shuffle=True),\n",
    "    'validation': DataLoader(data['validation'], batch_size=vm_batch_size, shuffle=True),\n",
    "}\n",
    "\n",
    "train_datagen1 = tf.keras.preprocessing.image.ImageDataGenerator(horizontal_flip=True,\n",
    "                                   width_shift_range=0.4,\n",
    "                                   height_shift_range=0.4,\n",
    "                                   zoom_range=0.3,\n",
    "                                   rotation_range=20,\n",
    "                                   rescale=1./255)\n",
    "\n",
    "train_gen1 = train_datagen1.flow_from_directory('train/train_data/train',\n",
    "                                              target_size=(249,249),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')\n",
    "\n",
    "test_datagen1 =  tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "test_gen1 = test_datagen1.flow_from_directory('valid/valid_data/valid',\n",
    "                                              target_size=(249,249),\n",
    "                                              batch_size=32,\n",
    "                                              class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_40k = torch.utils.data.Subset(data['training'], list(range(0,40000)))\n",
    "validation_10k =  torch.utils.data.Subset(data['validation'], list(range(0,10000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " xception (Functional)       (None, 2048)              20861480  \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 2048)             8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2048)              4196352   \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 2048)             8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              2098176   \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 1024)             4096      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,177,513\n",
      "Trainable params: 6,305,793\n",
      "Non-trainable params: 20,871,720\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from tensorflow.keras.applications import Xception\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "target_size = (249,249,3)\n",
    "#base\n",
    "model = Sequential()\n",
    "model.add(Xception(include_top=False, pooling='avg', weights='imagenet', input_shape=target_size))\n",
    "model.add(Flatten())\n",
    "model.add(BatchNormalization())\n",
    "#head\n",
    "model.add(Dense(2048, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.layers[0].trainable = False\n",
    "\n",
    "model.summary()\n",
    "model.compile(optimizer= 'Adam', loss = 'binary_crossentropy', metrics= 'accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Asked to retrieve element 0, but the Sequence has length 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [89]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n\u001b[0;32m     10\u001b[0m batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m---> 11\u001b[0m history1 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtest_gen1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_gen1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m                    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\preprocessing\\image.py:100\u001b[0m, in \u001b[0;36mIterator.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[0;32m     99\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m idx \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 100\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAsked to retrieve element \u001b[39m\u001b[38;5;132;01m{idx}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    101\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut the Sequence \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    102\u001b[0m                      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhas length \u001b[39m\u001b[38;5;132;01m{length}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(idx\u001b[38;5;241m=\u001b[39midx, length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m)))\n\u001b[0;32m    103\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    104\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtotal_batches_seen)\n",
      "\u001b[1;31mValueError\u001b[0m: Asked to retrieve element 0, but the Sequence has length 0"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint('xception_v1_{epoch:02d}_{val_accuracy:.3f}.h5',\n",
    "                                             save_best_only = True,\n",
    "                                             monitor= 'val_accuracy',\n",
    "                                             mode = 'max')\n",
    "\n",
    "\n",
    "epochs = 12\n",
    "batch_size=256\n",
    "history1 = model.fit(train_gen1, epochs= epochs, validation_data= test_gen1,\n",
    "                    steps_per_epoch= len(train_gen1)//batch_size,\n",
    "                    validation_steps= len(test_gen1)//batch_size,\n",
    "                    callbacks= [checkpoint]\n",
    "                    )\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
